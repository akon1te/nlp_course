{"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uDFhyOmQan0v"},"source":["# Лабораторная 3. Мяков, Шустров, Полякова"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{},"colab_type":"code","id":"TslNQEFlan0z"},"outputs":[],"source":["import os\n","import time\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.optim\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from tqdm.auto import tqdm\n","\n","from datasets import load_dataset\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/plain":["'cuda'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["device = \"cuda\" if torch.cuda.is_available() else 'cpu'\n","device"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"1z-nkq4Tan0-"},"source":["## 1. Char-RNN on Arxiv summaries"]},{"cell_type":"markdown","metadata":{},"source":["#### Dataset"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>link</th>\n","      <th>time</th>\n","      <th>favorites</th>\n","      <th>rts</th>\n","      <th>authors</th>\n","      <th>category</th>\n","      <th>published</th>\n","      <th>summary</th>\n","      <th>title</th>\n","      <th>tweeted</th>\n","      <th>summary_len</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>arxiv.org/abs/1611.10003</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[Tom A. F. Anderson, C. -H. Ruan]</td>\n","      <td>q-bio.NC</td>\n","      <td>2016-11-30 05:17:11</td>\n","      <td>In summary of the research findings presented ...</td>\n","      <td>Vocabulary and the Brain: Evidence from Neuroi...</td>\n","      <td>0</td>\n","      <td>1106</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>arxiv.org/abs/1611.10007</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[M. Amin Rahimian, Amir G. Aghdam]</td>\n","      <td>cs.SY</td>\n","      <td>2016-11-30 05:37:11</td>\n","      <td>In this paper, structural controllability of a...</td>\n","      <td>Structural Controllability of Multi-Agent Netw...</td>\n","      <td>0</td>\n","      <td>1390</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>arxiv.org/abs/1611.10010</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[Debidatta Dwibedi, Tomasz Malisiewicz, Vijay ...</td>\n","      <td>cs.CV</td>\n","      <td>2016-11-30 06:00:47</td>\n","      <td>We present a Deep Cuboid Detector which takes ...</td>\n","      <td>Deep Cuboid Detection: Beyond 2D Bounding Boxes</td>\n","      <td>0</td>\n","      <td>825</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>arxiv.org/abs/1611.10012</td>\n","      <td>2016-12-01 01:46:12</td>\n","      <td>11.0</td>\n","      <td>2.0</td>\n","      <td>[Jonathan Huang, Vivek Rathod, Chen Sun, Mengl...</td>\n","      <td>cs.CV</td>\n","      <td>2016-11-30 06:06:15</td>\n","      <td>In this paper, we study the trade-off between ...</td>\n","      <td>Speed/accuracy trade-offs for modern convoluti...</td>\n","      <td>1</td>\n","      <td>974</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>arxiv.org/abs/1611.10014</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>NaN</td>\n","      <td>[Yoones Hashemi, Amir H. Banihashemi]</td>\n","      <td>cs.IT</td>\n","      <td>2016-11-30 06:12:45</td>\n","      <td>In this paper, we propose a characterization o...</td>\n","      <td>Characterization and Efficient Exhaustive Sear...</td>\n","      <td>0</td>\n","      <td>1913</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                       link                 time  favorites  rts  \\\n","0  arxiv.org/abs/1611.10003                  NaN        NaN  NaN   \n","1  arxiv.org/abs/1611.10007                  NaN        NaN  NaN   \n","2  arxiv.org/abs/1611.10010                  NaN        NaN  NaN   \n","3  arxiv.org/abs/1611.10012  2016-12-01 01:46:12       11.0  2.0   \n","4  arxiv.org/abs/1611.10014                  NaN        NaN  NaN   \n","\n","                                             authors  category  \\\n","0                  [Tom A. F. Anderson, C. -H. Ruan]  q-bio.NC   \n","1                 [M. Amin Rahimian, Amir G. Aghdam]     cs.SY   \n","2  [Debidatta Dwibedi, Tomasz Malisiewicz, Vijay ...     cs.CV   \n","3  [Jonathan Huang, Vivek Rathod, Chen Sun, Mengl...     cs.CV   \n","4              [Yoones Hashemi, Amir H. Banihashemi]     cs.IT   \n","\n","             published                                            summary  \\\n","0  2016-11-30 05:17:11  In summary of the research findings presented ...   \n","1  2016-11-30 05:37:11  In this paper, structural controllability of a...   \n","2  2016-11-30 06:00:47  We present a Deep Cuboid Detector which takes ...   \n","3  2016-11-30 06:06:15  In this paper, we study the trade-off between ...   \n","4  2016-11-30 06:12:45  In this paper, we propose a characterization o...   \n","\n","                                               title  tweeted  summary_len  \n","0  Vocabulary and the Brain: Evidence from Neuroi...        0         1106  \n","1  Structural Controllability of Multi-Agent Netw...        0         1390  \n","2    Deep Cuboid Detection: Beyond 2D Bounding Boxes        0          825  \n","3  Speed/accuracy trade-offs for modern convoluti...        1          974  \n","4  Characterization and Efficient Exhaustive Sear...        0         1913  "]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["arxiv_csv = pd.read_csv('data/arxiv_papers.csv')\n","arxiv_csv['summary_len'] = [len(title[1]['summary']) for title in arxiv_csv.iterrows()]\n","arxiv_csv.head()"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["filtered_csv = arxiv_csv.loc[arxiv_csv['summary_len'] > 256]\n","train_csv = filtered_csv[:int(arxiv_csv.shape[0] * 0.7)]\n","val_csv = filtered_csv[int(arxiv_csv.shape[0] * 0.7):]\n","test_csv = arxiv_csv.loc[arxiv_csv['summary_len'] < 256]"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["((19031, 11), (7930, 11), (226, 11))"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["train_csv.shape, val_csv.shape, test_csv.shape"]},{"cell_type":"markdown","metadata":{},"source":["Arxiv dataset сначала отфилтрован по длине summary, все что больше 256 поделено на train / val в соотношении: <br>\n","70% - тренировка <br>\n","30% - валидация <br>\n","\n","Все что меньше 256 это test"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{},"colab_type":"code","id":"DXnbcOYT36sN"},"outputs":[],"source":["class ArxivDataset(Dataset):\n","    def __init__(self, dataframe: pd.DataFrame, chunk_len: int = 10):\n","        self.texts = dataframe['summary'].tolist()\n","        self.chunk_len = chunk_len\n","        self.all_symbols = set([])\n","        for text in self.texts:\n","            self.all_symbols.update({x for x in text})\n","        self.all_symbols = list(self.all_symbols)\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def _encode_vector(self, text: str):\n","        return torch.LongTensor([[self.all_symbols.index(item)] for item in text])\n","\n","    def _decode_vector(self, seq: str):\n","        seq = seq.view(-1).cpu().numpy()\n","        if seq.shape[0] == 1:\n","            seq = list(seq)\n","        return ''.join([self.all_symbols[x] for x in seq])\n","\n","    def __getitem__(self, idx: int):\n","        start_index = random.randint(0, len(self.texts[idx]) - self.chunk_len - 1)\n","        end_index = start_index + self.chunk_len + 1\n","        chunk = self.texts[idx][start_index:end_index]\n","        return self._encode_vector(chunk[:-1]), self._encode_vector(chunk[1:])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{},"colab_type":"code","id":"QO8snknfan1W"},"outputs":[{"name":"stdout","output_type":"stream","text":["Arxiv ds unique symbols:  97\n"]}],"source":["BATCH_SIZE = 16\n","CHUNK_LEN = 256\n","\n","full_dataset = ArxivDataset(arxiv_csv)  # for full vocab and generation\n","vocab = len(ArxivDataset(arxiv_csv).all_symbols)\n","print('Arxiv ds unique symbols: ', vocab)\n","\n","# train / val / test dataset for measure quality of model\n","train_dataset = ArxivDataset(train_csv, chunk_len=CHUNK_LEN)\n","val_dataset = ArxivDataset(val_csv, chunk_len=CHUNK_LEN)\n","test_dataset = ArxivDataset(test_csv, chunk_len=CHUNK_LEN)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=True)\n","test_dataloader = DataLoader(val_dataset, batch_size=1)"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{},"colab_type":"code","id":"uB9dp0eman1B"},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n","        super(RNN, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.embedding_size = embedding_size\n","        self.n_layers = n_layers\n","\n","        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n","        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.fc2 = nn.Linear(self.hidden_size, self.input_size)\n","\n","    def forward(self, x, hidden):\n","        x = self.encoder(x).squeeze(2)\n","        out, (ht1, ct1) = self.lstm(x, hidden)\n","        out = self.dropout(out)\n","        x = self.fc1(out)\n","        x = self.fc2(x)\n","        return x, (ht1, ct1)\n","\n","    def init_hidden(self, batch_size=1):\n","        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n","                torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))\n","\n","    @staticmethod\n","    def from_pretrained(filename):\n","        with open(filename, 'rb') as f:\n","            checkpoint = torch.load(f)\n","\n","        model = RNN(input_size=checkpoint['input_size'],  \n","                    hidden_size=checkpoint['hidden_size'], \n","                    embedding_size=checkpoint['hidden_size'],\n","                    n_layers=checkpoint['n_layers'])\n","        model.load_state_dict(checkpoint['state_dict'])\n","        return model\n","    \n","       \n","def save_model(model, filename='rnn.ckpt'):\n","    checkpoint = {'input_size': model.input_size,\n","                    'hidden_size': model.hidden_size,\n","                    'n_layers': model.n_layers,\n","                    'state_dict': model.state_dict()}\n","    with open(filename, 'wb') as f:\n","        torch.save(checkpoint, f)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{},"colab_type":"code","id":"7wCYn1gaan1e"},"outputs":[],"source":["n_layers = 3\n","embedding_size = 256\n","hidden_size = 256\n","\n","model = RNN(input_size=vocab,\n","            hidden_size=hidden_size,\n","            embedding_size=embedding_size,\n","            n_layers=n_layers).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n","lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def generate(model, dataset, inputs=' ', max_new_tokens=100, temp=0.3):\n","    hidden = model.init_hidden()\n","    input_ids = dataset._encode_vector(inputs).to(device)\n","    pred_seq = []\n","\n","    _, hidden = model(input_ids, hidden)\n","    input_ids = input_ids[-1].view(-1, 1, 1)\n","\n","    for _ in range(max_new_tokens):\n","        output, hidden = model(input_ids.to(device), hidden)\n","        logits = output.cpu().data.view(-1)\n","        probs = F.softmax(logits / temp, dim=- 1).numpy()\n","        next_id = np.random.choice(vocab, p=probs)\n","        input_ids = torch.LongTensor([next_id]).view(-1, 1, 1).to(device)\n","        pred_seq.append(next_id)\n","\n","    return inputs + dataset._decode_vector(torch.tensor(pred_seq))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["' 7KEMbchV+Q} L.Op\\x7f6A4J+302TPU;g%N)ZpJD.z}GEC5je:rMI}id\\nAXSDnGa\\\\e8[\"S~(kX+\"?E*@2zV3bgTZKA3a^@c$EPI!&m,'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["generate(model, full_dataset, inputs=' ', max_new_tokens=100)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["experiment = 'exps/final'\n","tb_writer = SummaryWriter(log_dir=experiment)  # for tensorboard logging\n","\n","NUM_EPOCHS = 10\n","\n","for epoch in tqdm(range(NUM_EPOCHS), desc='Epoch'):\n","\n","    epoch_loss = 0\n","    model.train()\n","    for input_ids, target in train_dataloader:\n","        input_ids = input_ids.permute(1, 0, 2).to(device)\n","        target = target.permute(1, 0, 2).to(device)\n","        hidden = model.init_hidden(BATCH_SIZE)\n","\n","        output, hidden = model(input_ids, hidden)\n","        loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n","        epoch_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    lr_scheduler.step()\n","\n","    tb_writer.add_scalar('Train loss', epoch_loss / len(train_dataloader), epoch)\n","\n","    ppl = []\n","    best_model, best_loss = None\n","    \n","    epoch_loss = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for input_ids, target in val_dataloader:\n","            input_ids = input_ids.permute(1, 0, 2).to(device)\n","            target = target.permute(1, 0, 2).to(device)\n","            hidden = model.init_hidden(BATCH_SIZE)\n","\n","            output, _ = model(input_ids, hidden)\n","            loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n","            ppl.append(loss.item())\n","        \n","        if np.mean(ppl) < best_loss:\n","            best_loss = np.mean(ppl)\n","            save_model(model=model, filename=f'{experiment}/model.ckpt')\n","            \n","        ppl = np.exp(np.mean(ppl))\n","        tb_writer.add_scalar('Perplexity val', ppl, epoch)\n","    \n","        \n","best_model = RNN.from_pretrained(filename=f'{experiment}/model.ckpt')\n","\n","ppl = []\n","model.eval()\n","with torch.no_grad():\n","    for input_ids, target in tqdm(test_dataloader):\n","        input_ids = input_ids.permute(1, 0, 2).to(device)\n","        target = target.permute(1, 0, 2).to(device)\n","        hidden = best_model.init_hidden(1)\n","\n","        output, _ = model(input_ids, hidden)\n","        loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n","        ppl.append(loss.item()) \n","\n","ppl = np.exp(np.mean(ppl))\n","tb_writer.add_scalar('Perplexity test', ppl, 0)"]},{"cell_type":"markdown","metadata":{},"source":["![Train loss](./img/arxiv_train_loss.png)"]},{"cell_type":"markdown","metadata":{},"source":["![Val perplexity](./img/arxiv_perplexity_test.png)"]},{"cell_type":"markdown","metadata":{},"source":["На картинке выше отображены train loss лучших отобранных эксеприментов. <br>\n","1) Почти во всех отобранных экспериментах используется LSTM (т.к. GRU или RNN показывает значительно более плохиер результаты из-за различии в лсожности архитекутр)\n","2) Размер эмбеддинга улучшают результат, но самые отптимальные судя по проведенным экспериментам 256\n","3) Размер фич в LSTM в 256 и 512 показывает лучшие результаты\n","4) 2 линейных слоя также показывают лучшие метрики\n","\n","Возможно какие то комбинации параметров не были здесь представленны, но после множества экспериментов модель:<br>\n","embeds (256 dim size) <br>\n","lstm (256 hidden_size) - 3 layers <br>\n","dropout 0.2 <br>\n","fully connected (256 hidden_size) - 2 layers <br>\n","\n","показала лучшие результаты"]},{"cell_type":"markdown","metadata":{},"source":["Эксперименты с температурой."]},{"cell_type":"markdown","metadata":{},"source":["1. Так как у нас идет генерация по буквам, то при низком значении температуры символы будут выбираться из самого большего по вероятности: "]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["In this paper and the set of the set of the set of the set of the set of the set of the set of the set of the set \n","In this paper and the set of the set of the set of the set of the proposed to the set of the set of the set of the\n"]}],"source":["print(generate(model, full_dataset, inputs='In this paper ', max_new_tokens=100, temp=0.01))\n","print(generate(model, full_dataset, inputs='In this paper ', max_new_tokens=100, temp=0.01))"]},{"cell_type":"markdown","metadata":{},"source":["2. При высокой же температуре вероятности будут почти нулевые и будет генерация будет из равновероятным значений"]},{"cell_type":"code","execution_count":45,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["In this paper +d|C0i{%\n","\n","2PA\"fmo!<2$\n","ba  KA\\tE(sjNAn^o_h/v/SGcD'YiT9^+4H\\^qJK|aKfgTIz;\\)|P1j3fee$0XDwj|fo}i.]misuca\n"]}],"source":["print(generate(model, full_dataset, inputs='In this paper ', max_new_tokens=100, temp=5))"]},{"cell_type":"markdown","metadata":{},"source":["2. Самой оптимальной температурой по наблюдения стала температура от 0.3 до 0.8"]},{"cell_type":"code","execution_count":46,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["In this paper and the control to an analysis of the set of the experiments control from the problem to the set of \n"]}],"source":["print(generate(model, full_dataset, inputs='In this paper ', max_new_tokens=100, temp=0.3))"]},{"cell_type":"code","execution_count":51,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["In this paper and extends show that the first surpreters in expression of different measurements, undering the dec\n"]}],"source":["print(generate(model, full_dataset, inputs='In this paper ', max_new_tokens=100, temp=0.6))"]},{"cell_type":"markdown","metadata":{},"source":["Но все равно в силу размеров модели сгененированный текст имеет нулевую смысловую нагрузку"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"k4-MqjYH55pT"},"source":["## 2. char-RNN on personal dataset\n"]},{"cell_type":"markdown","metadata":{},"source":["Интересно было взять какой то необычный датасет, чтобы визуально было видно, как хорошо работает наша RNN, поэтому мы решили взять [датасет с гороскопами](https://huggingface.co/datasets/dkagramanyan/horoscopes_ru) из-за особенности текстов, которые в нем присутствуют."]},{"cell_type":"code","execution_count":69,"metadata":{},"outputs":[],"source":["dataset = load_dataset(\"dkagramanyan/horoscopes_ru\")"]},{"cell_type":"code","execution_count":70,"metadata":{},"outputs":[],"source":["class HoroscopesDataset(Dataset):\n","    def __init__(self, dataset, chunk_len: int = 10):\n","        self.dataset = []\n","        for item in dataset:\n","            if len(item['text']) >= 258:\n","                self.dataset.append(item)    \n","        self.chunk_len = chunk_len\n","        self.all_symbols = set([])\n","        for item in self.dataset:\n","            self.all_symbols.update({x for x in item['text']})\n","        self.all_symbols = list(self.all_symbols)\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def _encode_vector(self, text: str):\n","        return torch.LongTensor([[self.all_symbols.index(item)] for item in text])\n","\n","    def _decode_vector(self, seq: str):\n","        seq = seq.view(-1).cpu().numpy()\n","        if seq.shape[0] == 1:\n","            seq = list(seq)\n","        return ''.join([self.all_symbols[x] for x in seq])\n","\n","    def __getitem__(self, idx: int):\n","        start_index = random.randint(0, len(self.dataset[idx]['text']) - self.chunk_len - 1)\n","        end_index = start_index + self.chunk_len + 1\n","        chunk = self.dataset[idx]['text'][start_index:end_index]\n","        return self._encode_vector(chunk[:-1]), self._encode_vector(chunk[1:])"]},{"cell_type":"code","execution_count":79,"metadata":{},"outputs":[],"source":["class RNN(nn.Module):\n","    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n","        super(RNN, self).__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.embedding_size = embedding_size\n","        self.n_layers = n_layers\n","\n","        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n","        self.lstm = nn.GRU(self.embedding_size, self.hidden_size, self.n_layers)\n","        self.dropout = nn.Dropout(0.2)\n","        self.fc1 = nn.Linear(self.hidden_size, self.hidden_size)\n","        self.fc2 = nn.Linear(self.hidden_size, self.input_size)\n","\n","    def forward(self, x, hidden):\n","        x = self.encoder(x).squeeze(2)\n","        out, ht = self.lstm(x, hidden)\n","        out = self.dropout(out)\n","        x = self.fc1(out)\n","        x = self.fc2(x)\n","        return x, ht\n","\n","    def init_hidden(self, batch_size=1):\n","        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n","                torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))\n","\n","    @staticmethod\n","    def from_pretrained(filename):\n","        with open(filename, 'rb') as f:\n","            checkpoint = torch.load(f)\n","\n","        model = RNN(input_size=checkpoint['input_size'],  \n","                    hidden_size=checkpoint['hidden_size'], \n","                    embedding_size=checkpoint['hidden_size'],\n","                    n_layers=checkpoint['n_layers'])\n","        model.load_state_dict(checkpoint['state_dict'])\n","        return model\n","    \n","       \n","def save_model(model, filename='rnn.ckpt'):\n","    checkpoint = {'input_size': model.input_size,\n","                    'hidden_size': model.hidden_size,\n","                    'n_layers': model.n_layers,\n","                    'state_dict': model.state_dict()}\n","    with open(filename, 'wb') as f:\n","        torch.save(checkpoint, f)"]},{"cell_type":"code","execution_count":80,"metadata":{},"outputs":[],"source":["BATCH_SIZE = 16\n","CHUNK_LEN = 256\n","\n","# train / val / test dataset for measure quality of model\n","train_dataset = HoroscopesDataset(dataset['train'], chunk_len=CHUNK_LEN)\n","val_dataset = HoroscopesDataset(dataset['test'], chunk_len=CHUNK_LEN)\n","vocab = len(train_dataset.all_symbols)\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, drop_last=True)"]},{"cell_type":"code","execution_count":81,"metadata":{},"outputs":[],"source":["n_layers = 3\n","embedding_size = 512\n","hidden_size = 512\n","\n","model = RNN(input_size=vocab,\n","            hidden_size=hidden_size,\n","            embedding_size=embedding_size,\n","            n_layers=n_layers).to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n","lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["experiment = 'exps/horoscopes/emb_512_gru_512_3l_2fc_dropout'\n","tb_writer = SummaryWriter(log_dir=experiment)  # for tensorboard logging\n","\n","NUM_EPOCHS = 10\n","\n","for epoch in tqdm(range(NUM_EPOCHS), desc='Epoch'):\n","\n","    epoch_loss = 0\n","    model.train()\n","    for input_ids, target in train_dataloader:\n","        input_ids = input_ids.permute(1, 0, 2).to(device)\n","        target = target.permute(1, 0, 2).to(device)\n","        hidden = model.init_hidden(BATCH_SIZE)[0]\n","\n","        output, hidden = model(input_ids, hidden)\n","        loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n","        epoch_loss += loss.item()\n","\n","        loss.backward()\n","        optimizer.step()\n","        optimizer.zero_grad()\n","    lr_scheduler.step()\n","\n","    tb_writer.add_scalar('Horoscopes: Train loss', epoch_loss / len(train_dataloader), epoch)\n","\n","    ppl = []\n","    best_model, best_loss = None\n","    \n","    epoch_loss = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for input_ids, target in val_dataloader:\n","            input_ids = input_ids.permute(1, 0, 2).to(device)\n","            target = target.permute(1, 0, 2).to(device)\n","            hidden = model.init_hidden(BATCH_SIZE)\n","\n","            output, _ = model(input_ids, hidden)\n","            loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n","            ppl.append(loss.item())\n","        \n","        if np.mean(ppl) < best_loss:\n","            best_loss = np.mean(ppl)\n","            save_model(model=model, filename=f'{experiment}/model.ckpt')\n","            \n","        ppl = np.exp(np.mean(ppl))\n","        tb_writer.add_scalar('Horoscopes: Perplexity val', ppl, epoch)\n","    \n","        \n","best_model = RNN.from_pretrained(filename=f'{experiment}/model.ckpt')\n","\n","ppl = []\n","model.eval()\n","with torch.no_grad():\n","    for input_ids, target in tqdm(test_dataloader):\n","        input_ids = input_ids.permute(1, 0, 2).to(device)\n","        target = target.permute(1, 0, 2).to(device)\n","        hidden = best_model.init_hidden(1)\n","\n","        output, _ = model(input_ids, hidden)\n","        loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n","        ppl.append(loss.item()) \n","\n","ppl = np.exp(np.mean(ppl))\n","tb_writer.add_scalar('Horoscopes: Perplexity test', ppl, 0)"]},{"cell_type":"markdown","metadata":{},"source":["Для выбранного корпуса решено было провести меньшее кол-во экспериментов на основе результатов с arxiv."]},{"cell_type":"markdown","metadata":{},"source":["![Train loss](./img/horo_train_loss.bmp)"]},{"cell_type":"markdown","metadata":{},"source":["![Perplexity test](./img/horo_perplexity_test.bmp)"]},{"cell_type":"markdown","metadata":{},"source":["На большом датасете с большим кол-во тренировочной выборки более большие модели (в плане кол-ва слое в параметров) показывают лучший резульата. Т.е модели с 512 hidden size показали результаты значительно лучше, чем с 256."]},{"cell_type":"code","execution_count":57,"metadata":{},"outputs":[{"data":{"text/plain":["'В понедельник получение в своей работы и стремление к развития контактах с детьми. Возможно, вам придется вам стоит подходящий момент для происходящего происходящее в этот день благоприятен для профессиональной сто'"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["generate(model, train_dataset, inputs='В понедельник ', max_new_tokens=200)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Lab6.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
