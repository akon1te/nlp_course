{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from html2text import html2text\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Парсинг URL адресов с новостями (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROME_DRIVER_PATH = \"tools/chromedriver.exe\"\n",
    "LINKS_FILE_PATH = \"data/habr_links.txt\"\n",
    "ARTICLES_PATH = \"data/habr_articles.json\"\n",
    "WEBSITE_LINK = \"https://habr.com/ru/hubs/\"\n",
    "PAGES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = Options()\n",
    "options.add_argument('headless')\n",
    "\n",
    "driver = Chrome(service=Service(executable_path=CHROME_DRIVER_PATH), options=options)\n",
    "print(\"Current session is {}\".format(driver.session_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_website_category(category, pages_cnt):\n",
    "    for page in range(1, pages_cnt + 1):\n",
    "        driver.get(WEBSITE_LINK + category + \"/articles/page\" + str(page))\n",
    "        for element in driver.find_elements(By.CLASS_NAME , \"tm-articles-list__item\"):\n",
    "            with open(LINKS_FILE_PATH, 'a') as file:\n",
    "                file.write(\n",
    "                    json.dumps({\n",
    "                        'link' : element.find_element(By.CLASS_NAME, \"tm-title_h2\").find_element(By.TAG_NAME, 'a').get_attribute('href'),\n",
    "                        'category': category,\n",
    "                    },\n",
    "                    ensure_ascii=False) + '\\n'\n",
    "                )   \n",
    "                \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current session is 831d8a8e4e96cffe34bd7439ceac0d47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsed categories: 100%|██████████| 6/6 [39:12<00:00, 392.02s/it]\n"
     ]
    }
   ],
   "source": [
    "categories = [\"machine_learning\", \"infosecurity\", \"gamedev\", \"maths\", \"webdev\", \"algorithms\"]\n",
    "\n",
    "for cat in tqdm(categories, desc=\"Parsed categorie\"):\n",
    "    parse_website_category(category=cat, pages_cnt=PAGES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Парсинг новостей (BeautifulSoup + html2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_symbols(text):\n",
    "    bad_symbols_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return bad_symbols_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(article):\n",
    "    soup = BeautifulSoup(requests.get(article['link']).text, \"lxml\")\n",
    "\n",
    "    title = soup.find(\"h1\", {\"class\": \"tm-title_h1\"}).select_one(\"span\").text\n",
    "    tags = [tag.contents[0] for tag in soup.find_all(\"a\", {\"class\": \"tm-tags-list__link\"})]\n",
    "\n",
    "    text = ''\n",
    "    findall_tags = ['p', 'h3', 'h4']\n",
    "    for paragraph in soup.find_all(name=findall_tags, attrs={'class': ''}):\n",
    "        text += html2text(paragraph.text).replace('\\n', ' ') + ' '\n",
    "    text = remove_bad_symbols(text)\n",
    "\n",
    "    return {\n",
    "        'article_id': article['link'],\n",
    "        'title': title,\n",
    "        'category': article['category'],\n",
    "        'tags': tags,\n",
    "        'text': text.strip(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_corpus = {'catalog': []}\n",
    "with open(LINKS_FILE_PATH) as file:\n",
    "    for article in file:\n",
    "        articles_corpus['catalog'].append(parse_article(json.loads(article))) \n",
    "        \n",
    "with open(ARTICLES_PATH, 'a', encoding='UTF-8') as file:    \n",
    "    json.dump(articles_corpus, fp=file, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
