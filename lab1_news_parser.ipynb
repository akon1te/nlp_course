{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from html2text import html2text\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Парсинг URL адресов с новостями (Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROME_DRIVER_PATH = \"tools/chromedriver.exe\"\n",
    "LINKS_FILE_PATH = \"data/habr_small/habr_links.txt\"\n",
    "ARTICLES_PATH = \"data/habr_small/habr_articles.json\"\n",
    "WEBSITE_LINK = \"https://habr.com/ru/hubs/\"\n",
    "PAGES = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_website_category(category: str, pages_cnt: int, links_storage: set):\n",
    "    options = Options()\n",
    "    options.add_argument('headless')\n",
    "    driver = Chrome(service=Service(executable_path=CHROME_DRIVER_PATH), options=options)\n",
    "    \n",
    "    with open(LINKS_FILE_PATH, 'a') as file:\n",
    "        for page in range(1, pages_cnt + 1):\n",
    "            driver.get(WEBSITE_LINK + category + \"/articles/page\" + str(page))\n",
    "            for element in driver.find_elements(By.CLASS_NAME , \"tm-articles-list__item\"):\n",
    "                    try:\n",
    "                        link = element.find_element(By.CLASS_NAME, \"tm-title_h2\").find_element(By.TAG_NAME, 'a').get_attribute('href')\n",
    "                        if link not in links_storage:\n",
    "                            links_storage.add(link)\n",
    "                            file.write(\n",
    "                                json.dumps({\n",
    "                                    'link' : element.find_element(By.CLASS_NAME, \"tm-title_h2\").find_element(By.TAG_NAME, 'a').get_attribute('href'),\n",
    "                                    'category': category,\n",
    "                                },\n",
    "                                ensure_ascii=False) + '\\n'\n",
    "                            )  \n",
    "                    except:\n",
    "                        pass \n",
    "                \n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"artificial_intelligence\", \"hr_management\", \"physics\", \"gadgets\", \"biotech\"]\n",
    "links_storage = set()\n",
    "for cat in tqdm(categories, desc=\"Parsed categories\"):\n",
    "    parse_website_category(category=cat, pages_cnt=PAGES, links_storage=links_storage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Парсинг новостей (BeautifulSoup + html2text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_bad_symbols(text):\n",
    "    bad_symbols_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "\n",
    "    return bad_symbols_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_article(article: dict):\n",
    "    soup = BeautifulSoup(requests.get(article['link']).text, \"lxml\")\n",
    "\n",
    "    title = soup.find(\"h1\", {\"class\": \"tm-title_h1\"}).select_one(\"span\").text\n",
    "    tags = [tag.contents[0] for tag in soup.find_all(\"a\", {\"class\": \"tm-tags-list__link\"})]\n",
    "    text = soup.find('div', {\"class\": \"article-formatted-body\"}).get_text()\n",
    "    text = re.sub(' +', ' ', text).replace('\\n', ' ').replace('\\r', '')\n",
    "    text = remove_bad_symbols(text)\n",
    "    \n",
    "    return {\n",
    "        'article_id': article['link'],\n",
    "        'title': title,\n",
    "        'category': article['category'],\n",
    "        'tags': tags,\n",
    "        'text': text.strip(),\n",
    "    } if len(text) != 0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_corpus = {'catalog': []}\n",
    "\n",
    "cat_dict = {i: 0 for i in categories}\n",
    "with open(LINKS_FILE_PATH) as file:\n",
    "    for article in tqdm(file.readlines()):\n",
    "        article_dict = json.loads(article)\n",
    "        article_info = parse_article(article_dict)\n",
    "        if article_info:\n",
    "            cat_dict[article_dict['category']] += 1\n",
    "            articles_corpus['catalog'].append(article_info) \n",
    "            \n",
    "articles_corpus['meta'] = cat_dict\n",
    "\n",
    "with open(ARTICLES_PATH, 'a', encoding='UTF-8') as file:    \n",
    "    json.dump(articles_corpus, fp=file, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
